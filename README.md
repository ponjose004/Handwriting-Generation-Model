# Handwriting-Generation-Model
This Project aims to Generate Handwriting using Machine Learning methods. We have LSTM and RNN concept.

ğŸ“ **HANDWRITTEN TEXT GENERATION**  
==================================  

ğŸš€ **PROJECT OVERVIEW**  
This pipeline takes any input text and turns it into realistic, human-like handwriting using deep learning.  

â€¢ **Data Preparation**: Splitting & normalizing the DeepWriting Dataset (https://ait.ethz.ch/deepwriting)  
â€¢ **Model Architectures**:  
  â€“ MDN-LSTM (Graves-style) for sequence-to-sequence stroke synthesis  
  â€“ SketchRNN-Style VAE for richer latent-space sampling  
â€¢ **Training & Evaluation**:  
  â€“ Teacher-forcing training with MDN negative-log-likelihood, BCE (pen flag) and KL losses  
  â€“ Validation metrics: pen-lift accuracy & average NLL  
â€¢ **Inference**:  
  â€“ Jupyter notebook & CLI for interactive demos  
  â€“ Scripted generation of handwriting images  

---  

ğŸ“‚ **DIRECTORY STRUCTURE**  
project_root/  
  data/                  Raw & split DeepWriting (.npz â†’ per-sample .npz)  
  models/                Saved checkpoints (best.pth, full_model.pth)  
  notebooks/             Jupyter notebooks for exploration  
  src/                   Core modules  
    data.py              Dataset & collate_fn  
    model.py             TextEncoder, Decoder, SketchRNN  
    loss.py              MDN loss, VAE+KL loss  
    train.py             Training loop & checkpointing  
    generate.py          Sampling & CLI code  
  README.txt             This file  

---  

ğŸ—ƒï¸ **DATA PREPARATION**  
1. Download DeepWriting:  
   â€¢ deepwriting_training.npz  
   â€¢ deepwriting_validation.npz  
2. Split into one .npz per sample (scripts/split_deepwriting.py)  
3. Normalize strokes using provided mean & std from the monolithic .npz  

---  

ğŸ—ï¸ **MODEL ARCHITECTURES**  

1. **MDN-LSTM (Alex Graves 2013)**  
   â€“ **Encoder**: Embedding â†’ Bidirectional LSTM â†’ summed forward/backward outputs  
   â€“ **Decoder**: Attention over encoder outputs + autoregressive LSTM on [Î”x, Î”y, pen_up] + context  
   â€“ **Mixture-Density Output**:  
     â€¢ M Gaussians â†’ (Î¼â‚“, Î¼áµ§, Ïƒâ‚“, Ïƒáµ§, Ï, Ï€)  
     â€¢ + Bernoulli logit for pen-lift  
   â€“ **Loss**: MDN negative-log-likelihood + binary cross-entropy for pen flag  

2. **SketchRNN-Style VAE**  
   â€“ **Encoder**: Bidirectional LSTM â†’ latent z (Î¼, log ÏƒÂ²)  
   â€“ **Reparameterize**: z âˆ¼ N(Î¼, ÏƒÂ²)  
   â€“ **Decoder**: LSTM conditioned on z at each step â†’ MDN + categorical pen-state output  
   â€“ **Loss**: MDN NLL + categorical CE + KL(q(z|x) â€– p(z))  

---  

ğŸ‹ï¸â€â™€ï¸ **TRAINING**  
â€¢ Script: src/train.py  
â€¢ Features: gradient clipping (â€–gâ€– â‰¤ 5), LR scheduling, validation loop, best-model checkpointing  
â€¢ Metrics: Pen-up accuracy, average MDN NLL  

---  

ğŸ” **EVALUATION**  
Run:  
  python src/eval.py --model models/best.pth --data_dir data/deepwriting/val  
Outputs pen-lift accuracy (%) and average MDN NLL per batch  

---  

ğŸ“¥ **DOWNLOAD LINKS**  
â€¢ DeepWriting Dataset: https://ait.ethz.ch/deepwriting  
â€¢ Pretrained Model (best.pth): https://your-domain.com/models/best.pth  

---  

ğŸ§‘â€ğŸ’» **CODE EXPLANATION**  
1. **data.py**  
   â€“ DeepWritingDataset: loads, normalizes & tokenizes strokes + text  
   â€“ collate_fn: pads strokes & text for batched training  

2. **model.py**  
   â€“ MDN-LSTM: TextEncoder, Attention, HandwritingDecoder  
   â€“ SketchRNN: VAE encoder, reparameterization, MDN decoder  

3. **loss.py**  
   â€“ mdn_loss: bivariate-Gaussian MDN + pen-lift BCE  
   â€“ sketchrnn_loss: MDN + categorical CE + KL divergence  

4. **train.py**  
   â€“ CLI with argparse, DataLoaders, tqdm progress bars, validation & checkpointing  

5. **generate.py**  
   â€“ generate_handwriting(): autoregressive sampling of Î”x, Î”y, pen  
   â€“ CLI prompt to convert text â†’ handwriting  

All code is modular and well-commentedâ€”feel free to tune hyperparameters, integrate style embeddings, or experiment with transformer-based variants! âœï¸ğŸ‰
