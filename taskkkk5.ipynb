{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f75b339a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting data/deepwriting\\deepwriting_training.npz → data/deepwriting\\train/\n",
      "Splitting data/deepwriting\\deepwriting_validation.npz → data/deepwriting\\val/\n"
     ]
    }
   ],
   "source": [
    "import os, numpy as np\n",
    "\n",
    "DATA_DIR = \"data/deepwriting\"\n",
    "SPLITS = {\n",
    "    \"train\":      \"deepwriting_training.npz\",\n",
    "    \"val\":         \"deepwriting_validation.npz\"\n",
    "}\n",
    "\n",
    "for split, fn in SPLITS.items():\n",
    "    inp = os.path.join(DATA_DIR, fn)\n",
    "    outdir = os.path.join(DATA_DIR, split)\n",
    "    os.makedirs(outdir, exist_ok=True)\n",
    "    print(f\"Splitting {inp} → {outdir}/\")\n",
    "    arr = np.load(inp, allow_pickle=True)\n",
    "    strokes = arr['strokes']\n",
    "    texts   = arr['texts']\n",
    "    for i, (s, t) in enumerate(zip(strokes, texts)):\n",
    "        s = s.astype(np.float32)\n",
    "        np.savez(os.path.join(outdir, f\"{i:06d}.npz\"),\n",
    "                 strokes=s, chars=t)\n",
    "    arr.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db4523e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Global mean: [1.0562458e-03 2.8255243e-05 4.2473756e-02] Global std: [0.00564527 0.00785551 0.19761197]\n"
     ]
    }
   ],
   "source": [
    "import os, numpy as np\n",
    "\n",
    "\n",
    "stats = np.load(os.path.join(DATA_DIR, \"deepwriting_training.npz\"), allow_pickle=True)\n",
    "global_mean = stats['mean'].astype(np.float32)  # shape (3,)\n",
    "global_std  = stats['std'].astype(np.float32)   # shape (3,)\n",
    "stats.close()\n",
    "print(\"Global mean:\", global_mean, \"Global std:\", global_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8bb6542",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class SketchRNNDataset(Dataset):\n",
    "    def __init__(self, folder, mean, std, max_len=250):\n",
    "        self.files = sorted(os.listdir(folder))\n",
    "        self.mean, self.std = mean, std\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self): return len(self.files)\n",
    "    def __getitem__(self, i):\n",
    "        data = np.load(os.path.join(folder, self.files[i]), allow_pickle=True)\n",
    "        strokes = data['strokes'][:self.max_len].astype(np.float32)\n",
    "        # normalize\n",
    "        strokes = (strokes - self.mean) / self.std\n",
    "        # pad to max_len\n",
    "        pad = self.max_len - strokes.shape[0]\n",
    "        if pad>0:\n",
    "            strokes = np.pad(strokes, ((0,pad),(0,0)), 'constant')\n",
    "        return torch.from_numpy(strokes)  # (max_len, 3)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # batch: list of (T,3) already fixed at max_len\n",
    "    return torch.stack(batch,0)  # (B,max_len,3)\n",
    "\n",
    "# example loader\n",
    "train_ds = SketchRNNDataset(os.path.join(DATA_DIR,'train'), global_mean, global_std)\n",
    "train_dl = DataLoader(train_ds, batch_size=64, shuffle=True, collate_fn=collate_fn, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a2f791e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 4. SketchRNN Model\n",
    "\n",
    "# %%\n",
    "import torch.nn as nn\n",
    "\n",
    "class SketchRNN(nn.Module):\n",
    "    def __init__(self, input_dim=3, hid_dim=256, z_dim=128, n_layers=1, n_mixes=20):\n",
    "        super().__init__()\n",
    "        # -- Encoder P(h|x)\n",
    "        self.enc_rnn = nn.LSTM(input_dim, hid_dim, n_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc_mu  = nn.Linear(2*hid_dim, z_dim)\n",
    "        self.fc_logvar = nn.Linear(2*hid_dim, z_dim)\n",
    "\n",
    "        # -- Decoder p(x|z)\n",
    "        self.z2h = nn.Linear(z_dim, hid_dim)\n",
    "        self.dec_rnn = nn.LSTM(input_dim + hid_dim, hid_dim, n_layers, batch_first=True)\n",
    "        self.fc_out = nn.Linear(hid_dim, n_mixes*6 + 3)  \n",
    "        # (6 params per Gaussian + pen_up, pen_down, pen_end)\n",
    "\n",
    "        self.hid_dim, self.z_dim, self.n_mixes = hid_dim, z_dim, n_mixes\n",
    "\n",
    "    def encode(self, x):\n",
    "        # x: (B, T, 3)\n",
    "        _, (h,_) = self.enc_rnn(x)  # h: (2*n_layers, B, hid_dim)\n",
    "        h = torch.cat([h[-2], h[-1]], dim=1)  # (B, 2*hid_dim)\n",
    "        mu     = self.fc_mu(h)\n",
    "        logvar = self.fc_logvar(h)\n",
    "        return mu, logvar\n",
    "\n",
    "    def reparameterize(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "\n",
    "    def decode(self, z, x):\n",
    "        # z: (B, z_dim), x: teacher-forced strokes (B, T, 3)\n",
    "        h0 = torch.tanh(self.z2h(z)).unsqueeze(0)  # initial hidden\n",
    "        c0 = torch.zeros_like(h0)\n",
    "        B,T,_ = x.size()\n",
    "        zrep = z.unsqueeze(1).expand(-1,T,-1)\n",
    "        inp = torch.cat([x, zrep], dim=2)  # (B, T, 3+z_dim)\n",
    "        out, _ = self.dec_rnn(inp, (h0,c0))\n",
    "        params = self.fc_out(out)  # (B, T, n_mixes*6+3)\n",
    "        return params\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar = self.encode(x)\n",
    "        z = self.reparameterize(mu, logvar)\n",
    "        params = self.decode(z, x)\n",
    "        return params, mu, logvar\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd9723f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# ## 5. Losses: Reconstruction + KL\n",
    "\n",
    "# %%\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def sketchrnn_loss(params, target, mu, logvar, n_mixes, kl_weight=0.5):\n",
    "    \"\"\"\n",
    "    params: (B,T,6*M+3)\n",
    "      - first 6*M for MDN\n",
    "      - last 3 are pen_up/down/end logits (use CE)\n",
    "    target: (B,T,3) with (dx,dy,one-hot pen state)\n",
    "    \"\"\"\n",
    "    B,T,_ = params.size()\n",
    "    # split params\n",
    "    mdn_params = params[:,:,:6*n_mixes]    # MDN part\n",
    "    pen_logits = params[:,:,6*n_mixes:]   # (B,T,3)\n",
    "    # MDN reconstruction over dx,dy\n",
    "    recon = mdn_loss(mdn_params, target[:,:,:3], n_mixes)\n",
    "    # pen state CE\n",
    "    pen_true = F.one_hot(target[:,:,2].long(), num_classes=3).float()\n",
    "    pen_loss = F.cross_entropy(pen_logits.view(-1,3), pen_true.view(-1,3), reduction='mean')\n",
    "    # KL divergence\n",
    "    kl = -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "    return recon + pen_loss + kl_weight*kl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36d63c3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Setup Device\n",
    "\n",
    "# %%\n",
    "import torch\n",
    "\n",
    "# choose GPU if available, otherwise CPU\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f6a690",
   "metadata": {},
   "source": [
    "FINAL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d77c659b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full training script with visible per‐epoch & per‐batch progress (no multiprocessing)\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# 0. Hyperparameters & device\n",
    "DEVICE     = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS     = 10\n",
    "LR         = 1e-3\n",
    "N_MIX      = 20\n",
    "MAX_SEQ    = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dc6dbeb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load global normalization stats\n",
    "stats = np.load(\"data/deepwriting/deepwriting_training.npz\", allow_pickle=True)\n",
    "GLOBAL_MEAN = stats['mean'].astype(np.float32)\n",
    "GLOBAL_STD  = stats['std'].astype(np.float32)\n",
    "stats.close()\n",
    "\n",
    "# 2. Vocabulary\n",
    "vocab    = ['<pad>','<s>','</s>','<unk>'] + [chr(i) for i in range(32,127)]\n",
    "char2idx = {c:i for i,c in enumerate(vocab)}\n",
    "\n",
    "# 3. Dataset & collate\n",
    "class DeepWritingDataset(Dataset):\n",
    "    def __init__(self, folder, char2idx, max_seq=MAX_SEQ):\n",
    "        self.files = sorted(os.listdir(folder))\n",
    "        self.folder = folder\n",
    "        self.c2i = char2idx\n",
    "        self.max_seq = max_seq\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path = os.path.join(self.folder, self.files[idx])\n",
    "        npz  = np.load(path, allow_pickle=True)\n",
    "        # 1. load strokes and text\n",
    "        s    = npz['strokes'][:self.max_seq].astype(np.float32)\n",
    "        txt  = npz['chars'].item()\n",
    "        npz.close()\n",
    "\n",
    "        # 2. normalize\n",
    "        s = (s - GLOBAL_MEAN) / GLOBAL_STD\n",
    "\n",
    "        # 3. tokenize\n",
    "        toks = ['<s>'] + list(txt) + ['</s>']\n",
    "        seq  = [self.c2i.get(t, self.c2i['<unk>']) for t in toks]\n",
    "\n",
    "        return torch.from_numpy(s), torch.tensor(seq, dtype=torch.long)\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    strokes, texts = zip(*batch)\n",
    "    B = len(batch)\n",
    "    lens_s = [s.shape[0] for s in strokes]\n",
    "    lens_t = [t.numel()   for t in texts]\n",
    "    T_max = max(lens_s); L_max = max(lens_t)\n",
    "\n",
    "    S = torch.zeros(B, T_max, 3)\n",
    "    for i, s in enumerate(strokes): S[i,:s.shape[0]] = s\n",
    "    T = torch.full((B, L_max), 0, dtype=torch.long)\n",
    "    for i, t in enumerate(texts):   T[i,:t.numel()]  = t\n",
    "\n",
    "    return {'strokes': S,\n",
    "            'seq_lens': torch.tensor(lens_s),\n",
    "            'texts':    T,\n",
    "            'text_lens':torch.tensor(lens_t)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3da3af8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Model components\n",
    "class TextEncoder(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, hid_dim=256, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, emb_dim, padding_idx=0)\n",
    "        self.lstm  = nn.LSTM(emb_dim, hid_dim, n_layers,\n",
    "                             bidirectional=True, batch_first=True)\n",
    "    def forward(self, txt, lengths):\n",
    "        e = self.embed(txt)\n",
    "        p = pack_padded_sequence(e, lengths.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        out,_ = self.lstm(p)\n",
    "        o,_   = pad_packed_sequence(out, batch_first=True)\n",
    "        H = o.size(2)//2\n",
    "        return o[:,:,:H] + o[:,:,H:]\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dec_dim, enc_dim):\n",
    "        super().__init__()\n",
    "        self.lin = nn.Linear(dec_dim + enc_dim, enc_dim)\n",
    "        self.v   = nn.Linear(enc_dim, 1, bias=False)\n",
    "    def forward(self, h_t, enc_out):\n",
    "        B,L,_ = enc_out.size()\n",
    "        h_exp = h_t.unsqueeze(1).expand(-1,L,-1)\n",
    "        e = torch.tanh(self.lin(torch.cat([h_exp, enc_out], dim=2)))\n",
    "        a = torch.softmax(self.v(e).squeeze(2), dim=1)\n",
    "        ctx = torch.bmm(a.unsqueeze(1), enc_out).squeeze(1)\n",
    "        return ctx, a\n",
    "\n",
    "class HandwritingDecoder(nn.Module):\n",
    "    def __init__(self, enc_dim=256, dec_dim=512, n_layers=2, n_mix=N_MIX):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(3 + enc_dim, dec_dim, n_layers, batch_first=True)\n",
    "        self.attn = Attention(dec_dim, enc_dim)\n",
    "        self.fc   = nn.Linear(dec_dim, n_mix*6 + 1)\n",
    "        self.n_mix = n_mix\n",
    "    def forward(self, strokes, enc_out):\n",
    "        B,T,_= strokes.size()\n",
    "        h=c=None; outs=[]\n",
    "        for t in range(T-1):\n",
    "            if t==0:\n",
    "                layers,_,hS = self.lstm.num_layers, B, self.lstm.hidden_size\n",
    "                h = torch.zeros(layers,B,hS, device=strokes.device)\n",
    "                c = torch.zeros_like(h)\n",
    "            prev = strokes[:,t]\n",
    "            ctx,_ = self.attn(h[-1], enc_out)\n",
    "            x = torch.cat([prev, ctx], dim=1).unsqueeze(1)\n",
    "            out,(h,c) = self.lstm(x,(h,c))\n",
    "            outs.append(self.fc(out.squeeze(1)))\n",
    "        return torch.stack(outs, dim=1)\n",
    "\n",
    "class HandwritingModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super().__init__()\n",
    "        self.enc = TextEncoder(vocab_size)\n",
    "        self.dec = HandwritingDecoder()\n",
    "    def forward(self, strokes, texts, text_lens):\n",
    "        enc = self.enc(texts, text_lens)\n",
    "        return self.dec(strokes, enc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d36add48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. MDN loss\n",
    "def mdn_loss(params, target, n_mix):\n",
    "    B,T,D = params.size()\n",
    "    p = params.view(B,T, n_mix*6+1)\n",
    "    log_pi = F.log_softmax(p[...,:n_mix], dim=-1)\n",
    "    idx = n_mix\n",
    "    mu_x = p[..., idx:idx+n_mix]; idx+=n_mix\n",
    "    mu_y = p[..., idx:idx+n_mix]; idx+=n_mix\n",
    "    log_sx = p[..., idx:idx+n_mix]; idx+=n_mix\n",
    "    log_sy = p[..., idx:idx+n_mix]; idx+=n_mix\n",
    "    rho    = torch.tanh(p[..., idx:idx+n_mix]); idx+=n_mix\n",
    "    pen_logits = p[..., -1]\n",
    "\n",
    "    x = target[...,0].unsqueeze(-1); y = target[...,1].unsqueeze(-1)\n",
    "    pen = target[...,2]\n",
    "    sx = torch.exp(log_sx); sy = torch.exp(log_sy)\n",
    "    nx = (x-mu_x)/sx; ny = (y-mu_y)/sy\n",
    "    z  = nx**2 + ny**2 - 2*rho*nx*ny\n",
    "    denom = 2*(1-rho**2)\n",
    "    log_coeff = - log_sx - log_sy - 0.5*torch.log(2*math.pi*(1-rho**2))\n",
    "    log_gauss = -z/denom + log_coeff\n",
    "\n",
    "    log_mix = log_pi + log_gauss\n",
    "    loss_pt = -torch.logsumexp(log_mix,dim=-1).mean()\n",
    "    loss_pen= F.binary_cross_entropy_with_logits(pen_logits, pen, reduction='mean')\n",
    "    return loss_pt + loss_pen\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df9bd192",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6. Instantiate datasets, dataloaders, model, optimizer\n",
    "train_ds = DeepWritingDataset(\"data/deepwriting/train\", char2idx)\n",
    "val_ds   = DeepWritingDataset(\"data/deepwriting/val\",   char2idx)\n",
    "train_dl = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    "    num_workers=0   # ← set to 0 in notebooks!\n",
    ")\n",
    "val_dl = DataLoader(\n",
    "    val_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    pin_memory=True,\n",
    "    num_workers=0\n",
    ")\n",
    "\n",
    "model = HandwritingModel(len(vocab)).to(DEVICE)\n",
    "opt   = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "sched = torch.optim.lr_scheduler.StepLR(opt, step_size=5, gamma=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a720d6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loader length: 1081\n",
      " Batch 0:  {'strokes': torch.Size([32, 334, 3]), 'seq_lens': torch.Size([32]), 'texts': torch.Size([32, 49]), 'text_lens': torch.Size([32])}\n",
      " Batch 1:  {'strokes': torch.Size([32, 341, 3]), 'seq_lens': torch.Size([32]), 'texts': torch.Size([32, 53]), 'text_lens': torch.Size([32])}\n",
      " Batch 2:  {'strokes': torch.Size([32, 343, 3]), 'seq_lens': torch.Size([32]), 'texts': torch.Size([32, 47]), 'text_lens': torch.Size([32])}\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train loader length: {len(train_dl)}\")\n",
    "for i, batch in enumerate(train_dl):\n",
    "    print(f\" Batch {i}: \", {k: v.shape for k,v in batch.items()})\n",
    "    if i >= 2: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f317de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "77926655",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1081/1081 [16:35<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Train loss: -33.7908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 23/23 [00:05<00:00,  4.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Val   loss: -41.5748\n",
      " ★ Saved best model (val=-41.5748)\n",
      "\n",
      "Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1081/1081 [16:25<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Train loss: -57.7069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 23/23 [00:05<00:00,  4.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Val   loss: -71.1862\n",
      " ★ Saved best model (val=-71.1862)\n",
      "\n",
      "Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1081/1081 [16:45<00:00,  1.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Train loss: -88.9167\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 23/23 [00:06<00:00,  3.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Val   loss: -108.1225\n",
      " ★ Saved best model (val=-108.1225)\n",
      "\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1081/1081 [19:36<00:00,  1.09s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Train loss: -128.4148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 23/23 [00:08<00:00,  2.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Val   loss: -141.3058\n",
      " ★ Saved best model (val=-141.3058)\n",
      "\n",
      "Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1081/1081 [16:36<00:00,  1.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Train loss: -159.5034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 23/23 [00:05<00:00,  4.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Val   loss: -162.3350\n",
      " ★ Saved best model (val=-162.3350)\n",
      "\n",
      "Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1081/1081 [17:36<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Train loss: -176.3723\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 23/23 [00:08<00:00,  2.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Val   loss: -178.4746\n",
      " ★ Saved best model (val=-178.4746)\n",
      "\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1081/1081 [20:15<00:00,  1.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Train loss: -195.4862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 23/23 [00:08<00:00,  2.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Val   loss: -197.0603\n",
      " ★ Saved best model (val=-197.0603)\n",
      "\n",
      "Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1081/1081 [21:36<00:00,  1.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Train loss: -217.6069\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 23/23 [00:05<00:00,  4.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Val   loss: -217.3119\n",
      " ★ Saved best model (val=-217.3119)\n",
      "\n",
      "Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1081/1081 [1:04:11<00:00,  3.56s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Train loss: -242.3370\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 23/23 [00:05<00:00,  4.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Val   loss: -243.2635\n",
      " ★ Saved best model (val=-243.2635)\n",
      "\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1081/1081 [17:34<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Train loss: -264.2079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Validating: 100%|██████████| 23/23 [00:08<00:00,  2.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " → Val   loss: -256.7216\n",
      " ★ Saved best model (val=-256.7216)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# 7. Training Loop with tqdm progress bars\n",
    "\n",
    "best_val = float('inf')\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total_train = 0.0\n",
    "    print(f\"\\nEpoch {ep}/{EPOCHS}\")\n",
    "\n",
    "    for batch in tqdm(train_dl, total=len(train_dl), desc=\"Training\"):\n",
    "        S, T, tL = batch['strokes'].to(DEVICE), batch['texts'].to(DEVICE), batch['text_lens'].to(DEVICE)\n",
    "        opt.zero_grad()\n",
    "        out  = model(S, T, tL)\n",
    "        loss = mdn_loss(out, S[:,1:], model.dec.n_mix)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
    "        opt.step()\n",
    "        total_train += loss.item()\n",
    "\n",
    "    sched.step()\n",
    "    avg_train = total_train / len(train_dl)\n",
    "    print(f\" → Train loss: {avg_train:.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    total_val = 0.0\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dl, total=len(val_dl), desc=\"Validating\"):\n",
    "            S, T, tL = batch['strokes'].to(DEVICE), batch['texts'].to(DEVICE), batch['text_lens'].to(DEVICE)\n",
    "            total_val += mdn_loss(model(S, T, tL), S[:,1:], model.dec.n_mix).item()\n",
    "\n",
    "    avg_val = total_val / len(val_dl)\n",
    "    print(f\" → Val   loss: {avg_val:.4f}\")\n",
    "\n",
    "    if avg_val < best_val:\n",
    "        best_val = avg_val\n",
    "        torch.save(model.state_dict(), os.path.join('models/', \"best.pth\"))\n",
    "        print(f\" ★ Saved best model (val={avg_val:.4f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2fdac93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pen-up accuracy: 26.26%\n",
      "Average MDN NLL per batch: -7818.9003\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## 10. Evaluate Pen-Up Accuracy & Avg NLL on Validation Set\n",
    "\n",
    "# %%\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. Create validation DataLoader (if not already)\n",
    "test_ds = DeepWritingDataset(\"data/deepwriting/val\", char2idx)\n",
    "test_dl = DataLoader(test_ds,\n",
    "                     batch_size=BATCH_SIZE,\n",
    "                     shuffle=False,\n",
    "                     collate_fn=collate_fn,\n",
    "                     pin_memory=True,\n",
    "                     num_workers=0)\n",
    "\n",
    "# 2. Load best model\n",
    "model.load_state_dict(torch.load(os.path.join('models/', \"best.pth\"), map_location=DEVICE))\n",
    "model.to(DEVICE).eval()\n",
    "\n",
    "# 3. Compute metrics\n",
    "correct = 0\n",
    "total   = 0\n",
    "total_nll = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for b in test_dl:\n",
    "        S  = b['strokes'].to(DEVICE)           # (B, T, 3)\n",
    "        T  = b['texts'].to(DEVICE)\n",
    "        tL = b['text_lens'].to(DEVICE)\n",
    "        out = model(S, T, tL)                  # (B, T-1, *)\n",
    "        \n",
    "        # Pen-up accuracy\n",
    "        pen_pred = (torch.sigmoid(out[..., -1]) > 0.5).float()\n",
    "        pen_true = S[:,1:,2]                   # ground-truth pen flags\n",
    "        correct += (pen_pred == pen_true).sum().item()\n",
    "        total   += pen_true.numel()\n",
    "        \n",
    "        # Average NLL (MDN loss)\n",
    "        batch_nll = mdn_loss(out, S[:,1:], model.dec.n_mix).item()\n",
    "        total_nll += batch_nll * S.size(0)\n",
    "\n",
    "# 4. Report\n",
    "pen_acc = correct / total\n",
    "avg_nll = total_nll / len(test_dl)\n",
    "print(f\"Pen-up accuracy: {pen_acc*100:.2f}%\")\n",
    "print(f\"Average MDN NLL per batch: {avg_nll:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c8e323b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pen-up accuracy: 26.26%\n",
      "Avg MDN NLL per batch: -7818.9003\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## A) Compute Pen-Up Accuracy & Average NLL on Validation Set\n",
    "\n",
    "# %%\n",
    "# 1. Build a test DataLoader (if not already)\n",
    "test_ds = DeepWritingDataset(os.path.join('data/deepwriting/', \"val\"), char2idx)\n",
    "test_dl = DataLoader(test_ds,\n",
    "                     batch_size=BATCH_SIZE,\n",
    "                     shuffle=False,\n",
    "                     collate_fn=collate_fn,\n",
    "                     pin_memory=True,\n",
    "                     num_workers=0)\n",
    "\n",
    "# 2. Load the best‐saved model weights\n",
    "model.load_state_dict(torch.load(os.path.join('models/', \"best.pth\"), map_location=DEVICE))\n",
    "model.to(DEVICE).eval()\n",
    "\n",
    "# 3. Iterate and accumulate metrics\n",
    "correct = 0\n",
    "total   = 0\n",
    "sum_nll = 0.0\n",
    "\n",
    "with torch.no_grad():\n",
    "    for b in test_dl:\n",
    "        S  = b['strokes'].to(DEVICE)        # (B, T, 3)\n",
    "        Tt = b['texts'].to(DEVICE)          # (B, L)\n",
    "        tL = b['text_lens'].to(DEVICE)      # (B,)\n",
    "\n",
    "        out = model(S, Tt, tL)              # (B, T-1, 6*N_MIX+1)\n",
    "\n",
    "        # Pen-up accuracy\n",
    "        pen_pred = (torch.sigmoid(out[..., -1]) > 0.5).float()\n",
    "        pen_true = S[:, 1:, 2]\n",
    "        correct += (pen_pred == pen_true).sum().item()\n",
    "        total   += pen_true.numel()\n",
    "\n",
    "        # MDN negative log-likelihood\n",
    "        batch_nll = mdn_loss(out, S[:, 1:], model.dec.n_mix).item()\n",
    "        sum_nll  += batch_nll * S.size(0)\n",
    "\n",
    "# 4. Report\n",
    "pen_acc = correct / total\n",
    "avg_nll = sum_nll / len(test_dl)\n",
    "print(f\"Pen-up accuracy: {pen_acc*100:.2f}%\")\n",
    "print(f\"Avg MDN NLL per batch: {avg_nll:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6bcb4ed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAADCCAYAAACYLLFmAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA5EklEQVR4nO3dd3jV5f3/8eeZOTkne+9BIIM9w5ANCigqigucrW2ttbXWouKoonUV7bdaa/VXrbOCExQnamTICnuFBAIhIXvvcZJzzuf3x0kCgQBJSHJOwvtxXbnOyZl3MOZ17vW+VYqiKAghhBDCodSOboAQQgghJJCFEEIIpyCBLIQQQjgBCWQhhBDCCUggCyGEEE5AAlkIIYRwAhLIQgghhBOQQBZCCCGcgASyEEII4QQkkIXoQUuXLuWGG24A4NChQ7i4uFBVVQVAVFQU8+fP79b3U6lULFu2rPX79evXo1KpWL9+fadfa9myZa3PzczMRKVScccdd3RbW4UQbUkgC9FDCgoKeOWVV5g5cyYAjz32GKNGjcLDw8PBLeuYmpoaVCoVQUFB1NTUABAcHOzgVgnRf0kgC9FDXnrpJerq6pgyZQqpqamsXr2aKVOmOLpZHbZx40YWLlxIfHw8GzduxGQycd999zm6WUL0WxLIQvSQgoICAEwmE4WFha3XT/fdd98xevRoXF1diY+P56233mr3te666y7CwsLQ6/VER0fz5JNPYrFYutS2NWvWMHHiRIxGI+7u7lx66aVs3bq19f6qqir27dvHU089BcC6deu49957CQwM7NL7CSHOT+voBgjRX73zzju88847gH2+uL2D1fbt28ef//xnli5dSmBgIG+++SZ33nknAwcOZOrUqYA9jBMTE1Gr1Tz++OPExMSwdetWnn76aTIzM3n77bc71a4VK1Zw8803c9lll7Fy5UrMZjPLly9n+vTpJCUlMXnyZDw8PDCbza3P+eSTT7r+DyGE6BAJZCEcqKSkhM2bNxMREQHA1KlTSUpKYsWKFa2BvGzZMsrLy0lJSWl93KxZs3B1dWXJkiU88MADDB48uEPvZ7PZeOCBBxg2bBjffvstarV9kOzyyy8nJiaGhx56iM2bN/fATyqEOB8ZshbCgUaOHNkasgAGg4HY2FiysrJab/vqq6+YMWMGISEhWCyW1q958+YBsGHDhg6/3+HDh8nLy+PWW29tDWMANzc3Fi5cyLZt26irq+uGn0wI0VnSQxbCgXx9fc+4zcXFhfr6+tbvCwsL+fLLL9HpdO2+RklJSYffr7S0FGh/tXRISAg2m43y8nKMRmOHX1MI0T0kkIVwcn5+fgwfPpxnnnmm3ftDQkI6/FotHwDy8/PPuC8vLw+1Wo23t3fXGiqEuCASyEI4ufnz5/PNN98QExNzwWEZFxdHaGgoK1asYMmSJahUKgBqa2v57LPPWldeCyF6n8whC+HknnrqKXQ6HZMmTeK1117jp59+4ptvvuHf//438+fPJycnp8OvpVarWb58OXv37mX+/PmsWbOGTz75hBkzZlBRUcHzzz/fgz+JEOJcpIcshJMLDg5m586d/PWvf+WFF14gJycHd3d3oqOjmTt3bqd7zYsXL8ZkMvHcc89x4403otFomDBhAuvWrWPSpEk99FMIIc5HpbS3OVIIIYQQvUqGrIUQQggnIIEshBBCOAEJZCGEEMIJSCALIYQQTkACWQghhHACEshCCCGEE5BAFkIIIZyABLIQQgjhBCSQhRBCCCcggSyEEEI4AQlkIYQQwglIIAshhBBOQAJZCCGEcAISyE7qRGkdj39xkEaLzdFNEUII0QskkJ2Qoig8svoA723NYvvxMkc3RwghRC+QQHZCX+3PZ9PREgw6NUlphY5ujhBCiF4ggexkqhua+OtXh5gzJJCFo8NISi1CURRHN0sIIUQPk0B2Mv/4IZ3qBguPXzmEWQkBnCir41hxjaObJYQQoodJIDuRlLxK3tlynD/OHkSolyuTYvzsw9apRY5umhBCiB4mgewkbDaFv3x+kBh/N+6cHA2AQadh8kA/ktIkkIUQor+TQHYSH+/MZveJCp5eMBSd5uR/lpnxgezKKqeirtGBrRNCCNHTJJCdQFltI89/l8a1o0MZP8C3zX0z4wOw2hQ2HCl2UOuEEEL0BglkJ/C3b9Ow2RQenpdwxn1BngaGhHjIPLIQQvRzEsgOtuVYCR/tzOaBufH4u7u0+5hZCYGsP1yExSpVu4QQor+SQHagstpG/vTRXibF+LI4MeKsj5sVH0BVg4VdWeW92DohhBC9SQLZQRRF4YFP9tFkVfjHjSPRqFVnfeywUE/83FxktbUQQvRjEsgO8s6WTJLSinjx+uEEehjO+Vi1WsXMeH+SUqWMphBC9FcSyL1sV1Y5G48U89w3afzikihmxgd26HmzEgI5VlxLZkltD7dQCCGEI2gd3YCLiaIoLHxtCwCDgz1YOi++w8+dPNAPvUZNUlpRa+EQIYQQ/Yf0kHtRXmVD6/VXFo/CRavp8HNNLlomxPjyk5z+JIQQ/ZIEci9Ky68CYPPSmcT4u3X6+bMTAkjOKKO6oam7myaEEMLBJJB7UVpBNR4GLSGe517EdTYz4gKw2BR+Ti/p5pYJIYRwNAnkXpSaX0V8sAcq1dm3OJ1LuI+RuEB3fpTV1kII0e9IIPeitIJqEoLcL+g1ZiYEsP5wMVab0k2tEkII4QwkkHtJQ5OVjOIa4oM9Luh1ZicEUFbbyN7siu5pmBBCCKcggdxLjhbVYFMg/gJ7yCPDvfE26mS1tRBC9DMSyL0kNb8KlQpiAy8skDVqFTPiAuT0JyGE6GckkHtJan41kT5GTC4XXotlZkIAaQXV5JTXdUPLhBBCOAMJ5F6SVlBFfNCFzR+3mBrrj1atYp0cNiGEEP2GBHIvUBSlecvThQ1Xt/Aw6EiM9nGa05/SC6v576bj1Jotjm6KEEL0WVLLuhccK66hvK6J0RHe3faaM+MDWL72MHWNFoz63v/PWFpjZs2+PFbtzuVAbiV6jZqZ8QFEd8OQvBBCXIzkr2cv2JZRhlatYkxk9wXy7IRAnv46lU3pJVw2JKjbXvdsViSf4JHVBwjyMKBWQVG1GUvzXmhXnYY3bhtLtJ+px9shhBD9lQxZ94JtGaUMC/PslgVdLaL8TAzwN/FTDw9bK4rCzswyHv/iIAAFVQ3kVTa0hjGAj0nPofxKdmSWUVkndbaFEKIrpIfcwxRFIfl4GQtHh3X7a8+KD+CLvXnYbApqddfKcbaw2RRKas0UVpopqGqgoKqBnLI6vkspIKu0DledBovN2u5zcyvqefabtDa3XTEsmFdvHn1BbRJCiIuJBHIPyyippbjazIQBPt3+2jPjA3nj5+MczKtkeJjXWR/X0GSloNIesoVVDW2u51c2UFjZ0GYIGkCrVhHg7sIlA/14/trhjI/2aRP69Y1WjpfU8v62TFZuzz7jPUd34/C8EEJcDCSQe1hyRhkatYqxUd0fyGOjvPEwaElKLTprIL+67igvrD3c5jZ3Fy2BngaCPAwM8HNjUowvQR4GAj0MBHu6Eujpgp/J5Zy9ble9hsEhHjx37XCeu3Y46YXVfLo7h9W7cymqNvPRjhNYbTYWjAwlwKNrp1sJIcTFRKUoipxS0IPuXbmHrLI6vrjnkh55/T+s3ENmSS1f/mHyGfel5ldx5SubuHFcOJcPCybQw0CQpwG3HlwJbbHa+PloCZ/tyuH7Q4VYrDamxfpz3ZhwZiUEYNBpeuy9hRCiL5Mecg+yzx+XsmBkaI+9x6z4AO77aC+FVQ0EntITtdoUHl51gGg/E09cOQS9tnfW72k1ambEBTAjLoDKuia+OpDHp7tyuGfFbjwMWq4aGcLC0WGMDPfq8jGU51NU1UCTTSHUy7VHXl8IIXqCBHIPyiyto7DKzIQBvj32HtNi/VGr4Ke0IhYlRrTe/kFyFnuzK/jktxN7LYxP52nUcfP4SG4eH8mx4ho+25XDqt25/G/bCWL8TSwcE8a1o8II8uyeIe3ssjpe23CMT3fm4GbQ8tUfJhMioSyE6CNk21MPSs4oRa2yz/X2FG+TnrGRPm0OmyiobGD5d4dZPD6CcT0wd90VMf5uPDg3ns1LZ/L+nYkMDfXk5R/TmfR8Erf+N5kv9ubS0NT+Ku7zySiuYckn+5j+4nq+O1jAH2YOxFWn4e4PdmO2dO01hegvjhRWs2xNCharzdFNEechPeQetC2jlKGhnrgbdD36PjMTAnj5x3QamqwYdBqeWHMQV72Gh+bG9+j7doVGrWLKIH/cXLQcL6llf04lP6eX8HN6SZvHjI7wQoV9SFvh5DKHU1c8KECjxUZKXiV+bi48cnkCixLDMeq1TI315/rXt/LXrw7x9IJhvfXjCeF09udU8s6WTAI8XPjd9IGObg75lfWsPVhAo9WGt1GPj0mPt0mPj9F+6WHQ9th0lrOTQO4hLfuP5w8P7vH3mhUfwPPfprE1o5RGi421KYW8ung0nq49+0HgQhj1WtwNWlx1GupP6xlbbQo7MssBmDDAhxAvY5v7W4Ia7NuzbhwXznVjwtosGBsR7sWyq4bwyOoDjI7w5toe2AcuRF/gbbT/HXjph3QuTQhk0AUeAdsVFXWNfH0gny/25LE9swy9Ro2LTk11w5n177VqFV5GPT4mHd5GPb5u+pPB3XzpZdTh6arDy6jH01WHh0GLVtP3B3wlkHvIibI68isbGB/dc/PHLQYGuBHhY+SLPblsyyhjZnwAlw/r+XKaFyIuyJ0PfjWBJquNlLwqdhwvY3tmGTszyyg/pdpX8vEyJsX4ct2YMOYMCepU3e5FieHsPlHOI6sPEB/kweCQ7jltS4i+xNukB0CnUbHk0/189tuJvRJeZouVdWnFrN6Tw7q0Yiw2G5MH+fP360dw6ZBAPAw6Gi02KuobKau1f5XXNlFW10h5y/d19sus0jrKaxsprW3EbGl/6N3dRYuHq+6UsLZferjq8HLVt7mt9cuow93FeXrksu2ph3y8I5uHVu1n7+OX9UpPddmaFN7ZkolRr+H7P00lzNt4/ic5IZtN4VhxDdszy9hxvIwdmeXkVtQDYNJruHFcBI9dkdDhymQNTVau/fcWahstrPn9ZKceNRCiJxwvqWXGi+uZMySQtSmFDA/z5M7J0TRZFZqsNixWG41WBYvVRpPVhtUGPm56QjwNhHi5EuLpiodrx0JLURR2ZZWzak8uX+/Pp7K+iWGhniwYFcpVI0Lwd3e54J+nvtFKWV0jlXVNVNa3fDVSWd9ERZvbmtrcVtXQRHtp52PS8+FvJhDrgJGD00kg95D7P97L4YJqvr53Sq+836b0Em75bzJ/mT+YOydH98p79pbcinp2Zpax/XgZNWYLL904slOfaE+U1jH/lZ9JjPbhP7eOveAyo0L0JbVmC6P/+sNZe5Y6jQqdRt38pUKlUlFe29imcp9RryH4lIAO9jK0XgZ7uqIoCl/uy2P13lyyy+oJ9XLl6pEhXDMq1CFD5O2x2RSqGywng7o5xF/+MR2NWsXn91zi8DoJEsg95JLnf2Lu0CD+Mn9wr7yfoihsPlrKxBhfNBI4Z0hKLeTOd3fywJw47pnh+IUtQvSmyrom6pusWBWFq/+1GV+Tni9+fwkuWnW7H26tNoWSGjO5FfXkVzSQX1lPXkUDeRX19uuVDRRXm9s8x91Fy+XDgrlmdCiJUT595oNvan4VV/9rM7dMiOTxK3vn7/XZyBxyD8guqyO3op7x0b235UilUjF5kF+vvV9fMyshkGtGhfJ/PxzhzsnRDv8kLERv8jTq8MQ+XfP/bh3Nda9v5d0tmdw1Labdx2vUKgKby+kS0e5DMFusFFaayausp6HJyoQBvn3y/6uEYA8emhfPX786xLQ4f6bF+jusLX1/WZoT2pZRikoFib0YyOLc8irq+eFQIdeNDuuTfzSE6C5jIn2485Jo/v7DEY4W1XT5dVy0GiJ8jUwY4Mv0uL5dFvcXk6KYGuvPkk/2UVpjPv8TeogEcg9IPl5GfJAHXka9o5sisA/nP7L6ACYXDY9ckeDo5gjhcEvmxBHm5coDn+7DapNZS7VaxYvXDcdqU3jw0/04aiZXArkHbMso7ZHjFkXXrNqdy/rDxTx7zTBZZS0EYNBpWH7dcPZmV/DWpuOObo5T8Hd3YVFiOElpRazZl+eQNsgccjfLKa8jp7y+V/Yfi/MrqmrgyS9TuGZUKLMSAh3dHCGcxtgoH355STQvfn+YmQkBxPi7ObpJDlHV0MSqXTm8vy2LY8W1DApwI9zHMdtGJZC7WXJGGSDzx85AURQe/fwgeq2Gx3tptbsQfcmSy+JISi3kgU/28clvJ11UOzQOF1Tz3tZMVu/JxWyxMWdIIE8vGMaEAT4OKxQigdzNko+XEh/kjo9J5o8d7av9+fxwqJDXbxndWq1ICHGSq17D8utGcON/tvL25uP8asoARzepRzVZbaxNKeC9rVlsP16Gv7sLv5oygMWJEd126tyFkEDuZtsyypgR57hl831ZZX0Tf/n8II9cnnDB/3OU1ph5Yk0KVwwLZu7Qnq8nLkRflRjtwx2Tonhh7WFmxgcwoB8OXRdWNbAi+QQrt5+gqNpMYpQPrywaxZwhQQ47nrY9EsjdKL+ynhNldT16/nF/9o8fjrBmXx6XDwtmrueF1eJ+6qtDKIrCk1cP6abWCdF/PTAnjp/Sinjw0/18dNfEfjF0rSgK2zLKeH9bJmtTCnHRqlkwKpTbJkYSH+Scde0lkLuRzB933eGCat7flgVAVX3TeR59bvuyK/hibx7LrxuOn9uF184Vor8z6rW80Dx0/c6WzD5dfrfGbGH1bvsirSOFNQzwN/GXKxK4dkwYHj18FO6FkkDuRtsySokNdMNXQqBTFEVh2ZoUIn2MFFQ1UNnBQK6oa6Sq3kKEb9sVkS+sPUxsoBsL5chFITosMdqH2ydG8cLaNGbGBxDtZ3J0kzolvdD+oX7V7lzqGi1cOjiQJ64cwqQYX6c5zel8JJC7UfLxMiYPlPKVnfXNgQK2ZpTyzi/G8ejqg1TUN573Oe9vy+Ivnx8EIPP5K1pv33y0hE1HS/jPrWP6xbCbEL3pwbktQ9f7+Og3E52+HrXFauOHQ4W8tzWLrRml+LnpuWNSFIvHRxDi5ero5nWaBHI3Kaxq4HhJLUsui3N0U/qU+kYrz3x9iNkJAUyPC8DTVXfOHrLFamPS8z9R1FzY/r1fJrbepygKy79LY1SEF5cOlj3HQnSWUa9l+XXDuek/23hnSya/dNKh66LqBj7cns2K5BMUVDUwJtKbl28aydyhQbho+24JT6cI5KqGJq58ZRPjo324fVIUQ0I8Hd2kTtuWUQrI/HFnvbb+KCU1ja2nYnm66qioaz+QU/OrmPfyz63fpzw5B5PLyV/htSkF7MupZOWvJ/SZISohnM2EAb7cPjGS5c1D11FOMnStKAo7s8p5b2sW3x3MR6tWs2BUCLdMiOyTmdEepwhkk16Lh0HHxztz+HhnDolR9mCeMyQQrcZ5lqSfy7aMMmL8Td1yAPfF4kRpHa9vzODXU6OJ9LX/T+9lPLOHrCgKr647yovfHwHgjklRLLuq7eppi9XGC2sPM2WQHxNjZJW7EBfiwbnx/HTYvur6w99McOjQda3Zwhd783hvayZpBdVE+RpZOi+B68aE9btSuE4RyBq1imevGcbVr24iMdoHmwL3rNhNsKeBWyZEctO4cKdfKJV8vJSJst2pU57++hC+Jn2b84k9XXXkVdS3fl9Z18Sjnx/gq/35AKz5/SUMD/M647VW7cnlWHEtL904qsfbLUR/Z3LR8reFw1n8RjLvbc3kjkt6f+j6WHEN72/N4rNdOdQ2WpgZH8jDlycwZaCf089td5VTBDLAsDBPbpsYxUc7svnxz9OorGvi3S2Z/DMpnZeT0rlyeAh3TIpiWFjPDU38MykdL6OO2yZGdep5RVUNZBTXct/s2J5pWD+08Ugx3x8q5JVFozDqT/4aehp1VDT3kLdllHL/R3vJq2wAYNmVg9sN44YmKy/9cIQrhgX36O+HEBeTSTF+3Dohkr99d5gZ8QGto1g9yWK1kZRWxPtbs9h0tAQfk55bJkayODHCYfWle5NTjQf/+bJYPFy1PPFFCoNDPPjbdcPZ9vAs/jQ7lm0ZpVz5r00sfG0La/bl0WS1det7V9Y18eq6o617iTsj+bj9ORNk/rhDGi02ln2ZwvhoH+YPb1tFy9NVR2lNI8u/S2PRG9sI8zGSGO1DkIeBmxLbPyn9g+QTFFabuf8y+UAkRHdaOi8eXzc9D366H1sPHtNYUmPm1XVHmbp8HXe9v4sas4X/u2EEW5bO5KG58RdFGIMT9ZAB3A06ll05hLs/2M3alALmDAnC26Tn7ukx/HpKND+mFvHulkzuXbmHQA8Xbh4fyaLEiG6Zt/1in73AeFfWAm3LKGWAn4kAD8fXQu0L3t2SSWZJLa8uHn3G4qvy2kZqzBb+38YMHpgTx6UJgcx5aSPLrhrS7gHoNWYLr647yvVjwi7a02qE6CkmFy3LFw5n8ZvJ/C85q9Ojh+eiKAq7T1Tw/tZMvjlQgEoFV40I4baJPTsS6sycKpAB5g4NYmZ8AMvWpHDJQD/cmlfRajVq5g4NYu7QINIKqnh3Sxb/Xn+Uf/10lF9OjmbpvPguv6eiKKzcng3Qpb2rycfLGC/zxx1SVNXAy0np3DIhkoTgk+XrFEXhox3ZvPGz/WzWd3+RyORBfvzxwz0Eehi4cVx4u6/35s8Z1Jgt/HH2oF5pvxAXm0kD/bhlQgTPfZPG9NiAMwrxdFZ9o5U1+3J5b2sWKXlVRPgYWTInluvHhF/0h8A41ZA1gEql4smrhlBe18g/fjjS7mPigzx47tphJD88myGhHmw+WtLl9yuvbeTeD/eSml+FQadG08kucnG1maNFNUwYIMPVHfH8d2noNCruv7Tt8PLalAKWrjrQ+n2kr5GjRdWs2ZfH72YMbHdvYWmNmTc2ZnDHpCiCPfteEQAh+oql8xLwMel58LN9XR66ziyp5emvDjHhuSSWrjpAoIeBt38xjvVLpvObqTEXfRiDE/aQAcJ9jNw3O5bl36VxzahQhoa2P3zhadRRUmNmdhcPnv8+pYBHVh+kyWrj5ZtG8v7WrE7vXy2ssi846o0FD31dWkEVq3bncsekqDO2K0we5M/d02N4bf0xAGb9fQONVhveRh03jD1ZAlNRFMwWGw1NVl5OSketUnH3tJhe/TmEuNi4udgLhtz8ZjIfJGdxaweHrq02hXVpRby/LYsNR4rxMuq4aVw4N4+PvOCedn/klIEMcOfkaFbvzuXR1QdY9btL2h1KLqhsILusnsSozvVOK+uaePLLFFbtyWV2QgDPXjOMAA8D72zJpLPbnlvmr8tqzZ174kXIx6hnVIQX72zJJK2gitER3jQ02WiwWGlotNJgsRLlaySztI7G5kV75XVNxD32HQAGnZqGpraL+f58aax8shaiF1wy0I/F4yN47ts0pscFnHOhVVltIx/tyOaD5CxyyusZHubJC9cN58oRIe2uBRF2ThvIOo2aZ68dysLXtrLiLJ/ItmfaVzeP7UQg/5RWyNLPDlDfZOXv14/g2tGhrb1imwLqTvaQfZrDoLhaAvl8AjwMrLp7EkmpRfxr3VHW7MvDVafBoNPgqtPgolMT5Gkgs7Su3ec3NNkYHeHFdWPC8THp8HDVMSFa5u6F6C0Pz4tnw+FiHvx0Px/8avwZ+4H3Zlfw3tbM1roB84cH86/FUYwM93JAa/sepw1kgDGRPixKDGf5d4eZMyTojFXMO46XEe3X8epYT36ZwtubM5ke58/z1w4nyLPt69lsSqc3nOs0anxMekpqzn8ggrCvEZg9OJDZZ6k1bbMpDHz0G2wKPHftMBYlRtBktZGUWsiK7dn8nF5MemENC0aFsigxot8WCBDCGbkbdPxt4XBu+W8yK7af4JYJkQBkl9Vxz4rd7M+pJNTLlT/NjuXGceGtHRbRMU4dyAB3TxvIyu3Z/JxewsIxbY/T25FZxrgo7w69TlFVA29vzuRPs2O5d9bAdueKbYpCV/6++7u5SA+5m6jVKjxcdbi5aLmu+b+3TqNm7tBg5g4NJrusjo92ZPPRzmze35bFiHAvFieGM394SJu61kKInjF5kB+LEiN47ptUpsX6E+5jJNDDQKSviT/OGsT0uAA5aa2LnG6V9ek2phejUauYMqjtsYaVdU0cLqxmXAeHqzeml6BSwS0TIs66cMtqUzq9yhrAz10vgdyNbhgbzlNXD0HXzoR+uI+RJXPi2LJ0Jq/fMgYvVx1LVx1g/LNJPPb5AVLyKh3QYiEuLo9cHo+nq46lq/ajKAp6rZpXFo1iVkKghPEFcPouxSe7cpgW63/GcPWuE2UoSsdPV9pwpJhhoZ7nrIltUzo/ZA32HnJLeUdx4R65POG8j9Gdsi89u6yOj3dm89GObP637QQjwjxZlBjBlSOk1yxET3A36Hh+4XBue2s7K7af4ObxkY5uUr/g1D3k9MJq9mVXtA5dnmr78XIC3F2I6EBJNatN4ef0YqbF+p/zcV1Z1AXg5+ZCifSQHSbcx8ifL7P3mv9z6xi8TXoeXn2AxGd+5JHVBziYK71mIbrb1Fh/bhoXzrNfp5JT3v5CTNE5Tt19+HRXDl5GHbMSAs64zz5/7NOhfcMHciupqGs6fyDblC4Nt/i7yxyyM9Bq1Fw2JIjLhgSRU17Hx81zzSuSTzD8lF6zm/SahegWj1yRwIYjxSz97ADv35ko55BfIKftIVusNlbtyeXqESFnVGlqaLKyP6eiwwu6Nhwuxt2gPe/Se5uidKmWtb+7C9VmCw1N1s4/WfSIMG8j918Wx+aHZvLGbWPxNel5ZPUBxj/zIw+vOsCBHOk1C3GhPAw6nrt2GJuOlvDhjmxHN6fPc9quwsb0YoqrzVw/9swaxnuzK2iyKozr8PxxEVMG+aE9T9UPq9LFRV3N89LF1eaL5lSSvkKrUXPp4EAuHRxIbkU9H+3I5uMd2azcfoJhofZe81UjpdcsRFdNjwvghrFhPPN1KlNj/Qn1kjK2XeW0PeRPd+UQH+TOkBCPM+7bmVmGu4uW+KAz7ztdcbWZPdkVTI89c9j7dDZb1+aQW/ZBF9fIsLUzC/Vy5f5LY9n00AzevG0sAe4uPPa5fa754VX72Z9T4egmCtEnPTZ/MO4GLUs/s6+6Fl3jlN2C8tpGfjxUxINz49qdk9ieWc6YKO8OzfeuTSlArVJx6VkKUZyqy6usmwNZFnb1DVqNurU4SV5Lr3lnNiu3ZzM01MPeax4RgrtBd/4XE0LgYdDx7LXD+MXbO/hoR/ZZzy4X5+aUPeQv9uZiUxQWjAo94z6rTWF3VnmH9x9/d7CAiQN8O1Tv2GrrWmEQb6MetUp6yH1RiJcrf7o0lp8fnMF/bx9LkIeBv3x+kPHPJrH0s/3sy66QT/xCdMCMuACuHxPG01+nkltR7+jm9ElOGcif7s5hRnxA69xsfaOVL/bmklVaS2p+FTVmS4cCuay2ka0ZpcwbFtSh97UpdGkOWaNW4SvVuvo0rUbNrIRA3rx9HJuXzuSuqTFsPFLM1a9u5op/buL9bVlUNTQ5uplCOLXH5g/G5KLh4VUH5INsFzjdkHVqfhUHc6u4d+Yg6hutfJCcxesbMiipMeOq0zDA337MYVyg+3lf64dDBSiKwmWDOxrIXRuyBntxkBLpIfcLwZ6u/HH2IH4/cyAbjhSxIjmbJ744yLNfp3LViBAWjY9gRJinbPEQ4jServZV1798Zyef7MzhhnFnLsoVZ+d0gfzprhzcXLQcL6nl0c8PUlbbyMLRofxycjQrkk/w3tYsAL4+kM/i8eeep/j2YAHjonw6fPiEvZZ11/7I+sle5H5Ho1YxMz6QmfGB5FfW88nOnNY62gnBHixODOfqUaF4yFyzEK1mxgeycHQYf/3qEFNi/Qj2lFXXHeVUQ9ZNVhuf78mlxmxh+drDJEb58NgVCfiYXHh3SyYVdU24N29PeXXdUWy2sw+JVNY3sfloCZcPC+7w+1ttSqfPQ24hB0z0b8Gertw7axAbH5zB23eMI8zblWVfHmL8M0k8+Ok+9pwolyE6IZo9Pn8wRhm67jSn6iEXV5sprbUfY+jnpmdtSgFfH8gn2NNAgLsLRr2WxGgfcivqSSuoZkN6MTPi2t/OlJRaSJNVYc6Qjg1XQ/Pxi13sIfu7u7Cj+Xxm0X9p1CpmxAcwIz6AgsoGPtmZzYc7svl4p32b3uLxEVw9MhRPV+k1i4uXp/Hk0PWnu3LarSchzuTwQM4qreXn9BI2pZew5VgJAO4uWkaEeTFlkB+TB/kT5WtsM1+nKAoLXt3Mf38+ftZA/uZAAWMivc848/hculrLGuwfIIqrzSiKInOLF4kgTwN/mDWI380YyM/pxaxIPsGTXx7i2W9SuXK4fa55VLiX/D6Ii9LM+ECuHR3KU18dYsog/079Lb5YOSyQ39+ayX9+ziC7rB6NWsXoCC/unDyAyYP8GBHmec6qWiqVijunDODelXtIza8iIbhtgZAas4WN6cU8OCeuU22ydvE8ZLD3kOubrNQ2WqXq00VGo1YxPS6A6XEBFFbZe80rt2fzSXNxm0WJESwYJb1mcfF5Yv4QNqWX8PCq/bx1xzj5cHoeDksOd4OOmXEBTB7kz4QBPp0uwjBvaBAhngbe2nScF64f0ea+n9KKaLTYmDu048PVYO95d/Usz1OLg0ggX7wCPQz8fuYgfjd9ID8fLWFl8gme+uoQz32byhXDQlg8PpzREd7yh0lcFDyNOp69Zhi/em8nn+3ObffkPnGSw5JjwajQdgt/dJROo+b2SVH8/fsjPDA3jgD3k8Mh3x7IZ0SYJ2HenasrbbVd2LYnsBcHifIzdek1RP+hVquYFuvPtFh/iqoa+GRXDiu3n+Cz3TnEBbqzKDGca0aF4WmUXrPo32YPDuSaUaE8+WUKkwf6ydD1OTjVKuvOuikxAq1Gxf+at0IB1DVaWHe4iHmdWF3d4kLmkFvrWctKa3GaAA8D98wYyMYHZvDeLxMZ4G/i6a9TSXz2R+7/eC87M8tkJaro1564cjAuWg2PrJZV1+fSpwPZ01XHDWPDeX9bVusB2RsOF9PQZGNeJ4erW7ZQdaVSV0tbdBqVFAcRZ6VWq5ga689rt4xhy8Mz+ePsQezMLOe617cy56WNvL35OJV1Ug1M9D9eRj3PXjOUn9KKWL0n19HNcVp9OpAB7po2ADeDlitf2cSWoyX8lFZEXKA7kb6dGza2Nn9q6+rUnkqlwk/2IosOCnA38LvpA1m/ZDrv35nIwAA3nmnpNX+0lx3Saxb9zGVDgrh6ZAjL1qRQVNXg6OY4pT4fyMGerqy5ZzJDQjy59a3tfHMgn0sG+nX6dWzNf/y6uqgL7MPWEsiiM9RqFVMG+fPvm8ew9eFZ3Dc7lp1Z5Vz/+lYu/cdG/rvpOBV1jY5uphDdYtmVQ9DL0PVZ9flABvA26XnnF+O4c3I0tY1WZsT7d/o1bDb7ZVfnkEHqWYsL4+/uwt3TY1i/ZDr/u3M8cYHuPPdNKonPJvGnj/ay/bj0mkXf5m3S88w1Q/kxtYjP98rQ9en6zf4crUbNI5cn8Ksp0W1WXHdUSw+5q6usAfzcXEgrqOry84UA++/g5EF+TB7kR3G1mc9221dor96TS4y/iUWJESwcHdahI0WFcDZzhgRx1YgQlq05xCUxfgR4yKrrFv2ih3yqroQxnJxD7uqiLpAha9H9/N1d+O20GNb9eTorfjWe+GAP/vZdGuOfS+K+D/eQnFEqvWbR5yy7agg6jYpHVh+U399T9Jse8oVqWWV9AR1k/N1dKKlplPKZotup1SomDfRj0kA/SmrMfNa8r/nzvXkM8DexODGCa0eH4SO9ZtEH+Jj0PL1gKL/9327W7Mvj6pFdr0nRn/S7HnJXtRwcdaFD1o1WG1X1lm5qlRBn8nNz4a5pMaxbMp0Vvx7PkBBP/vZdGhOeTeLelXvYekx6zcL5zR0azPzhwTyxJoWiall1DRLIraytPeQLG7IGKK6RXy7R81QqFZNi/Hhl0Si2PTyLJXNiOZBbyaI3tjHr7xt4Y2MGZbWyQls4ryevGoJGpeIxGboGJJBbKa3bnrr+Gt7NZRBLa+SPoOhdvm4u/GZqDD/9eRorfz2BoaGevLD2MBOeTeIPK/ew5ViJ/METTsfXzYWnFwzl+0OFrNmX5+jmOJzMITc7WRik6z3k9KIagE4XJRGiu6hUKibG+DIxxpey2kZW7c5hxfYTLH4jj2g/E4sSw1k4Ogzf5trrQjjavGHBXNE8dD0pxq91pPFiJD3kZi1zyBeyynpHZhnhPq5SPF04BR+Tnl9NGUDS/dP48DcTGB7myYtrjzDhuSR+v2I3W46WtC5mFMKRnrpqCGqViqe+OuTopjiUBHIzWzfMIe/MLGdclE93NUmIbqFSqZgwwJeXbxpF8iOzeGhuPKn5VSx+M5mZf1/P6xuOSUEb4VC+bi78ZX4CX+7LY11akaOb4zASyM1aF3V18V+kxmwhJa9SAlk4Ne/mXvOP90/j47smMirCm//74QgTn0vinhW72Sy9ZuEgC0aGMmWQH499fpBa88W5U0UCuZntAguD7D1RgU2BcVHe3dksIXqESqUiMdqHf9w4kuSHZ7F0XgKHC6q5+c1kZvx9Pa+tPyZFbkSvUqlUPLNgGKW1Zv7xwxFHN8chJJCbXWjpzB2ZZXgbdcT4u3Vns4Tocd4mPXdOjuaHP03lk99OZEyEN//48QiTnk/ing92syldes2id0T4GrlvdixvbT7OgZxKRzen10kgN2stDNLFHvLOrDLGRPpIhS7RZ6lUKsZF+fB/N45k+yOzeHheAkcKq7nlv8lMf3E9/15/VAo4iB535+Ro4oI8WLpqPxarzdHN6VUSyM2sF1A6s8lqY8+JChmuFv2Gl1HPLydH8/2fpvLpbycyNsqbl39MZ9JzP3H3/3ax8Uix9JpFj7ApCpE+RlLyqsgoqXV0c3qV7ENu1hLIXTkPOTW/irpGK2NlQZfoZ1QqFWOjfBgb5cMT84eweo99X/Ntb20n3MeVm8ZFcP3YsC4f6iLEqYqrzdz9v13sz6nkxetHEBvo7ugm9SoJ5GbKBQxZ78gsx0WrZmioRze3Sgjn4WnUcccl0dw+KYrdJ8pZkZzNP5PS+ccPR5idEMii8RFMGeh3QfXgxcUrJa+SX7+7k0arwsrfTGBM5MU34iiB3KylUldXAnlnZhkjwr1w0Wq6u1lCOB2VSsWYSB/GRPrw+PzBfL43lxXJJ7j9re2EebuyKDGC68eEyTm3gkaLjQO5lYyO8Drn+ppvD+Rz/8f7iAkw8Z9bxxLi5dqLrXQeEsjNWrc9dfLTvaIo7Mgs58ZxYT3RLCGcmqdRx+2TorhtYiS7T1SwcvsJXvkpnf/74QizEwJYlBjBxBhfzBYbDY1W6pvsXw1NNuobrTQ0f1/f2HL7yeunf69CRYSvkShfE1G+RiL9TAR7GKRH7qR2nyhn6Wf7OVJYw/PXDuOmxIgzHmOzKfzzp3Re+jGdK4YH8+J1I3DVX7wdGwnkZl09DzmrtI6SGrPMH4t+p67Rwo+pRdSaLR0KzPome+gadBoamppYm1LI2pTCDr+fWgVGvRaDToOrXo2rToOrTtP8vQarTWFtSgE55fWtaz70WjWRPkYifU1E+7Vcmoj0NRLs6dqlNSE9objazOGCaqyKQoC7C/7uLvgY9f3yw0St2cKL3x/mnS2ZDAv1ZN7QIJ788hDjon3abAuta7Sw5JN9fHOggD9fGsvvZw686HepSCA3O1mpq3O/EDsyy1CpYHTExTffIfq3bRml3Ltyz1nv16hVDApwI9zHSKCrrk142q+rOVxQw+d7c1v//3I3aPnlJdFcNiQQk16Lq775OToNOo2qQ3+QGy02civqySypJbO0lqzSOo6X1PLDoUKyTwvrCB8jUc296kg/U+v1EK+eCWuzxcrRohpS86tJy68iraCatIIqSto5AU6jVuHnpifA3YC/u0trUJ96veU+g65v9BrXHy7i0dUHKa018+jlCdwxKYpGq435/9zEfR/u5bO7J6HXqsmtqOfX7+4ks7SW128Zw9yhQY5uulNQKXImGwBbj5Wy6I1trFsynWi/jp/W9NCn+9mXU8F3903twdYJ4RjF1WYyimvIKKm1XxbXklFSy4myupMh66Il2t/EAD8TA/zdGOBvYoCfG9F+ptbhx6qGJr7Yk8sHySdIK6gm1MuVG8eFc8PY8G49jKXJaiO3vJ7M0trmwK5rDe3ssjosLWGtURPu40qUr4lgLwP+bobWMPR3d8HPTY+f29mDUFEUCqvMpOZXkVpQRVq+PXiPFde2/rtE+BhJCHYnPsiDhGB34oI80GlUFFebKao2t7m0fzXYL2vMNFnb/ll2N2jtbXNzIcDD0Hxp/97f/eR1bwf1ustqG3nqyxQ+35vHlEF+PLNgGBG+xtb7D+RUcs2/N/PrqQOYFR/Ab/+3CxethjdvH0tCsCyGbSGB3Gzz0RJufjOZjQ/MaPOLdD4z/76eS2L8+OuCoT3YOiGcS6PFxomyOjKKazhWXNsmtMvrmgBQqcCo06DXqtFp1Oi1avQaNTkV9TRa7AUfNGoVN4wN57lrh/V4my3W5p51aV2b3nVBZQPFNWZKa8ycvrXaqNfgom1ue3P7dRo1BVUNVDT/nO4uWuKbg7flMi7IHTeXrg1A2mwKlfVNFNeYKaoyU1zTYL88NcBrzBRVNVDV0Lbms1atwu/0sG79oNG2J94dvW5FUfhibx5PfXUIq03hL/MHs3B0aLsjHa+tP8bytWlo1SpGhXvz2i2j5RjQ08iQdTNb63nIHX9OaY2ZjOJa/jhrUA+1SgjnpNeqGRjgxsCAM0vFltc2klFiD+qq+iYarTaaLAqNVitNVoVGiw2zxUaT1UajxUZQL63G1mrURPqaiPQ1MS3W/4z7rTaFstrG1sArqTZTXtdIY3M7W77MFhsB7i7EB3sQH+ROmLdrt859qtUqvE16vE368+7DbWiytra3bY/b3ts+lF/FhiP22yy2M3vdpw+Ntx02t9/mbdS1+/PllNfx6OqDbDhSzPzhwTxx5ZBznmX8m6kDSMmrxNek59ErBqPXSl2q00kgN+tKYZCdWeUAcsKTEKfwNukZY7Jvi+pLNGpVaxj1FQadhnAfI+E+5x7Vs9kUKuqbmkO74Yzh8qLqBlLyKimqNlN9Wq9br1UT7GkgxNOVYC8DQR4Gfkwt5EhhDQAv3zSSq0eGnretGrWKfy0e3fUf9iIggdysK4VBdmaWEerletHumRNC9A1qtQofkx4fk564oI71uouae9qFVWbyKurJq2zgh5RCqk87GvGPH+7lsc8PEuLpSoiXgWAvV0K9XO0h7uVKiKcrQZ4G6RF3gARys66ch7wjs5yxUr9aCOEA9Y1WXlh7GJui4Oaixc2gtV+2fBnOvO6iVZ93eL29XrfZYuXVn47yncVKjL+JZ64ZRoSPsTWo8yrqya+oJ7eigX3ZFXx7IL91LQHYpwL93FyaA9oe1K2B3Xybn5tLv9wG1hkSyM2snTwPub7RysHcShaOkYIgQoje12ixsTG9mKNFNa23qVWcsTDtVFq1CjeDFpNei3tzSJuaA9u95bqL/b6W64cLqvnXuqMAXDsqlIfmxeNj0qPTqM85OljfaCWvsp78iobm4K63B3dlA+sOF5Ff0UB9k7X18TqNiqDmoXF7UBsI9mzubXvZw9vDoLvwfzgnJoHcrCWGS2sbO7Tyb292BRabIic8CSEcwtOoY+19U/nhUCGvbzjG3uwKYgPduX1SFNPj/GloslHTYKHG3PLVRI3Z2nxbE7VmK9XN1yvrmsgpr6emoYmKuiZKa8/cNw2wak8uq/bkAva5ZffmMDfpTwv10wLezaAlNtCd0RHerb11k4uWRouN7LI6TrR8ldaRVVbHwZQCahutZ7y/m4u2NahbetZjo3yYGOPbo//WvUW2PTWrbmhi/iubMOm1rPrdpPNuCXglKZ3//JzBvscvu+iHWYQQjqUoCsnHy3h9wzHWHy4m1MuVOydHc1NiOEb9uftdpTVmdp+oYFdWObuzytmXU4HZYkOvURMT4MaMOH8WjAqlrvFkmNuDvak56K322xpOuW62tAn8hqaeO9d4RLgXX9xzSY+9fm+SQD7Fobwqrvn3Zq4aEcLy64afc67ltre2o1HB279I7MUWCiHEuaXmV/GfjRms2ZeHu0HLbROjuGNSFD4mPTabQnpRDbuyyu0BfKKc481nDge4uzA2ypvREd6MifRmSIhnty3Eslht9oBu7pnXmJuobrDQ0GRFrVKh06jRalRo1Wp0GhVajRqt+uTtOnXz/c2PaXObumMV3voCCeTTfLorhyWf7DtrMXSwLwAb8eT33D09hntmDOzlFgohxPnllNfx5s/H+WhHNgoKI8O9SMmrorrBgkatIiHYnTER3oyOtAdwqFf37qcWnSeB3I5HVh/g0105fPbbSQwL8zzj/pS8Sq745yY+vmsiidF9a6+lEOLiUl7byHtbsziYV8mIME9GR3ozIswLUxcriYmeI4HcDrPFyvWvb6W0ppGv/jAZb5O+zf3vbsnkma9T2b/ssrPONUct/ZrfTY/hwbnxvdFkIYQQfZzs1G6Hi1bDv28eTW2jhfs+2tt6NGOLHZllDAvzPGsY72qu4HUgt7LH2yqEEKJ/kEA+izBvIy/fNIqN6cX886f01tsVRWFHZtk5C4K0HFn37DU9XzBfCCFE/yCBfA7TYv25b1YsLyels/5wEQA55fUUVpkZd5Y6vYqikFtRD3De+rJCCCFECwnk8/jDzIH2YP5oL9lldezMKgNgTGT7PeStx0oBmDLIr9faKIQQou+TQD4PtVrFSzeOxM1Fy+8+2M2m9FIGBbidsdCrxQOf7gfg0SsSerOZQggh+jgJ5A7wMup5/ZYxHC6s5rPdOYw9y3GLZou1dbg67jznmAohhBCnkkDuoKGhniy7cghgX2XdnnVp9nnmKYP8ZIO9EEKITpFA7oTF4+2Vu44W1XCwnS1Nf/0qFYC7p8f0aruEEEL0fRLInfTBr8YDMP+VTVSect5nRV1j63D1uLMMaQshhBBnI4HcSZcM9MPf3X484/0fnywa8tX+fAAmDvBFp5F/ViGEEJ0jydEFTy8YCkBSWhGvbTgGwGvr7ZeLxrd/IIUQQghxLlJdvAsuTQhkgJ+JjJJa/v79YbyMutbh6mmx/g5unRBCiL5IeshdoFar+NWUAQCEeLny6OqDAIyK8MLTVefIpgkhhOijJJC76NrRofi56RkWevJ4xiuHhziwRUIIIfoyCeQuMug03DEpirUpBa23XT4s2IEtEkII0ZdJIF+AWyZEYtBpCPIwcOzZywnyNDi6SUIIIfooWdR1AbyMep68aggGnQaNWipzCSGE6DqVoiiKoxshhBBCXOxkyFoIIYRwAhLIQgghhBOQQBZCCCGcgASyEEII4QQkkIUQQggnIIEshBBCOAEJZCGEEMIJSCALIYQQTkACWQghhHACEshCCCGEE5BAFkIIIZyABLIQQgjhBCSQhRBCCCcggSyEEEI4AQlkIYQQwgn8fxfqCO5hOvCdAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 600x200 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Define Generation Function\n",
    "\n",
    "# %%\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_handwriting(model, text, char2idx, device, max_steps=300):\n",
    "    model.eval()\n",
    "    n_mix = model.dec.n_mix\n",
    "\n",
    "    # 1. Tokenize\n",
    "    seq  = ['<s>'] + list(text) + ['</s>']\n",
    "    idxs = [char2idx.get(c, char2idx['<unk>']) for c in seq]\n",
    "    T    = torch.tensor(idxs, device=device).unsqueeze(0)\n",
    "    tL   = torch.tensor([len(idxs)], device=device)\n",
    "\n",
    "    # 2. Initial stroke input\n",
    "    prev    = torch.tensor([[0., 0., 1.]], device=device, dtype=torch.float32).unsqueeze(0)\n",
    "    strokes = [prev.view(-1).cpu().numpy()]\n",
    "\n",
    "    h = c = None\n",
    "    enc = model.enc(T, tL)\n",
    "\n",
    "    # 3. Autoregressive sampling\n",
    "    for _ in range(max_steps):\n",
    "        if h is None:\n",
    "            nl, _, hs = model.dec.lstm.num_layers, 1, model.dec.lstm.hidden_size\n",
    "            h = torch.zeros(nl, 1, hs, device=device)\n",
    "            c = torch.zeros_like(h)\n",
    "\n",
    "        ctx, _      = model.dec.attn(h[-1], enc)\n",
    "        inp         = prev.view(1, 3)\n",
    "        x_in        = torch.cat([inp, ctx], dim=1).unsqueeze(1)\n",
    "        out, (h, c) = model.dec.lstm(x_in, (h, c))\n",
    "        p           = model.dec.fc(out.squeeze(1)).squeeze(0)\n",
    "\n",
    "        # sample mixture component\n",
    "        pi_logits = p[:n_mix]\n",
    "        pi_probs  = torch.softmax(pi_logits, dim=0).detach().cpu().numpy()\n",
    "        comp      = np.random.choice(n_mix, p=pi_probs)\n",
    "\n",
    "        # means & stds\n",
    "        mu_x   = p[n_mix + comp].item()\n",
    "        mu_y   = p[2*n_mix + comp].item()\n",
    "        log_sx = p[3*n_mix + comp].item()\n",
    "        log_sy = p[4*n_mix + comp].item()\n",
    "        sx, sy = np.exp(log_sx), np.exp(log_sy)\n",
    "\n",
    "        # sample deltas\n",
    "        dx = mu_x + sx * np.random.randn()\n",
    "        dy = mu_y + sy * np.random.randn()\n",
    "\n",
    "        # sample pen-up\n",
    "        pen_prob = torch.sigmoid(p[-1]).item()\n",
    "        pen_up   = float(torch.bernoulli(torch.tensor(pen_prob)))\n",
    "\n",
    "        prev = torch.tensor([[dx, dy, pen_up]], device=device, dtype=torch.float32).unsqueeze(0)\n",
    "        strokes.append(prev.view(-1).cpu().numpy())\n",
    "\n",
    "        if pen_up == 1.0 and len(strokes) > 60:\n",
    "            break\n",
    "\n",
    "    # 4. Render\n",
    "    pts = np.cumsum(np.array(strokes), axis=0)\n",
    "    plt.figure(figsize=(6,2))\n",
    "    plt.plot(pts[:,0], -pts[:,1], linewidth=1)\n",
    "    plt.axis('off')\n",
    "    plt.title(f\"“{text}”\")\n",
    "    plt.show()\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Generate Handwriting\n",
    "\n",
    "# %%\n",
    "# Load best model if needed\n",
    "model.load_state_dict(torch.load(os.path.join('models/', \"best.pth\"), map_location=DEVICE))\n",
    "model.to(DEVICE)\n",
    "\n",
    "user_text = input(\"Enter text to generate handwriting: \")\n",
    "generate_handwriting(model, user_text, char2idx, DEVICE)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
